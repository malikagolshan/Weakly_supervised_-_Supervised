{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dc9765-3067-4124-98d3-e62b36cce962",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available(): \n",
    "    print(f\"{torch.cuda.device_count()} GPUs available.\")\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import networkx as nx\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from livelossplot import PlotLosses\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('text', usetex=False)\n",
    "plt.rc('font', family='serif')\n",
    "\n",
    "# ### Set seeds for reproducibility\n",
    "#seed = 15\n",
    "#torch.manual_seed(seed)\n",
    "#torch.cuda.manual_seed(seed)\n",
    "#np.random.seed(seed)\n",
    "#import random\n",
    "#random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d595114-a232-46bf-bf02-c723706f9ad4",
   "metadata": {},
   "source": [
    "## Test workflow on a single stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c80a7fa-f6fc-4261-950b-4ee1a202805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subset(df): \n",
    "    # Fit a line to the subset using linear regression\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    model = LinearRegression()\n",
    "    stream_ϕ = df[df.stream]['ϕ'].values.reshape(-1, 1)\n",
    "    stream_λ = df[df.stream]['λ'].values\n",
    "    model.fit(stream_ϕ, stream_λ)\n",
    "    \n",
    "    # Get the slope and intercept of the fitted line\n",
    "    slope = model.coef_[0]\n",
    "    intercept = model.intercept_\n",
    "    \n",
    "    # Function to calculate perpendicular distance from a point to a line\n",
    "    def distance_to_line(x, y, slope, intercept):\n",
    "        return abs(slope * x - y + intercept) / np.sqrt(slope**2 + 1)\n",
    "    \n",
    "    # Define the distance threshold\n",
    "    max_star_distance = np.max(distance_to_line(df[df.stream]['ϕ'].values, df[df.stream]['λ'].values, slope, intercept))\n",
    "    radius = np.max([2.0, max_star_distance])\n",
    "    \n",
    "    # Calculate distances of all points to the fitted line\n",
    "    distances = distance_to_line(df['ϕ'].values, df['λ'].values, slope, intercept)\n",
    "    \n",
    "    # Select points within the distance threshold\n",
    "    selected_points = df[distances <= radius]\n",
    "\n",
    "    return selected_points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0edb223-7488-430a-bf64-2f82f42944af",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_streams = sorted(glob(\"/global/cfs/cdirs/m3246/mpettee/misc/GaiaCWoLa/gaia_data/mock_streams/*.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15171fa7-fd72-44ad-ae5b-be73778466bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.load(mock_streams[1]), columns = [\"μ_δ\", \"μ_α\", \"δ\", \"α\", \"b-r\", \"g\", \"ϕ\", \"λ\", \"μ_ϕcosλ\", \"μ_λ\", 'stream'])\n",
    "df['α'] = df['α'].apply(lambda x: x if x > 100 else x + 360)\n",
    "df['stream'] = df['stream']/100\n",
    "df['stream'] = df['stream'].astype(bool)\n",
    "df = df[[\"ϕ\", \"λ\", \"μ_ϕcosλ\", \"μ_λ\", 'b-r', 'g', 'stream']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da7c97c-83a5-485f-ab0f-b6f1984d8808",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_points = create_subset(df)\n",
    "\n",
    "plt.figure(dpi=200, figsize=(3,3))\n",
    "plt.scatter(df[~df.stream]['ϕ'], df[~df.stream]['λ'], color='grey', alpha=0.5, marker=\".\");\n",
    "plt.scatter(selected_points['ϕ'], selected_points['λ'], color='crimson', alpha=0.5, marker=\".\");\n",
    "plt.scatter(df[df.stream]['ϕ'], df[df.stream]['λ'], color='black', alpha=0.5, marker=\".\");\n",
    "plt.xlabel(r\"$\\phi$ [$^\\circ$]\")\n",
    "plt.ylabel(r\"$\\lambda$ [$^\\circ$]\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a84f8c-6ad2-4ea0-9f74-06d7137c88e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_points.stream.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05804cdb-4b5c-41a1-8db2-1fabcf1d1b96",
   "metadata": {},
   "source": [
    "## Turn many streams into graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6d8c76-a9cd-4c53-98a1-4080cb1c007c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def create_mock_stream_graph(file, k=0):\n",
    "    df = pd.DataFrame(np.load(file), columns = [\"μ_δ\", \"μ_α\", \"δ\", \"α\", \"b-r\", \"g\", \"ϕ\", \"λ\", \"μ_ϕcosλ\", \"μ_λ\", 'stream'])\n",
    "    df['α'] = df['α'].apply(lambda x: x if x > 100 else x + 360)\n",
    "    df['stream'] = df['stream']/100\n",
    "    df['stream'] = df['stream'].astype(bool)\n",
    "    \n",
    "    ### create subset of stars\n",
    "    df = create_subset(df)\n",
    "\n",
    "    ### create node features and node labels\n",
    "    x = torch.tensor(df[[\"ϕ\", \"λ\", \"μ_ϕcosλ\", \"μ_λ\", 'b-r', 'g']].values, dtype=torch.float)\n",
    "    y = torch.tensor(df[['stream']].values, dtype=torch.long).squeeze()\n",
    "\n",
    "    ### no edges \n",
    "    # edge_index = torch.empty((2, 0), dtype=torch.long) #torch.tensor([[0, 1], [1, 2]], dtype=torch.long)\n",
    "    \n",
    "    ### create edges \n",
    "    # from torch_cluster import knn_graph # not working; use sklearn for now\n",
    "    # edge_index = knn_graph(x, k=k, loop=False)\n",
    "\n",
    "    ### Use scikit-learn to find k-nearest neighbors\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='auto').fit(x)\n",
    "    distances, indices = nbrs.kneighbors(x)\n",
    "    \n",
    "    ### Create edge_index tensor\n",
    "    row_indices = np.repeat(np.arange(len(x)), k)\n",
    "    col_indices = indices[:, 1:].flatten()  # Skip the first column (self-loops)\n",
    "    edge_index = torch.tensor([row_indices, col_indices], dtype=torch.long)\n",
    "\n",
    "    return Data(x=x, edge_index=edge_index, y=y)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66991cec-f13c-4954-99f4-a8e32b954059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def create_mock_stream_graph(file, k=0):\n",
    "    df = pd.DataFrame(np.load(file), columns = [\"μ_δ\", \"μ_α\", \"δ\", \"α\", \"b-r\", \"g\", \"ϕ\", \"λ\", \"μ_ϕcosλ\", \"μ_λ\", 'stream'])\n",
    "    df['α'] = df['α'].apply(lambda x: x if x > 100 else x + 360)\n",
    "    df['stream'] = df['stream'] / 100\n",
    "    df['stream'] = df['stream'].astype(bool)\n",
    "    \n",
    "    ### Create subset of stars\n",
    "    df = create_subset(df)\n",
    "\n",
    "    ### Create node features and node labels\n",
    "    x = torch.tensor(df[[\"ϕ\", \"λ\", \"μ_ϕcosλ\", \"μ_λ\", 'b-r', 'g']].values, dtype=torch.float)\n",
    "    y = torch.tensor(df[['stream']].values, dtype=torch.long).squeeze()\n",
    "\n",
    "    ### Use scikit-learn to find k-nearest neighbors\n",
    "    neigh = NearestNeighbors(n_neighbors=k+1, algorithm='auto')\n",
    "    neigh.fit(x)\n",
    "    knn = neigh.kneighbors(x, return_distance=False)\n",
    "\n",
    "    ### Create edges (explicit loop-based method)\n",
    "    start_edges = []\n",
    "    end_edges = []\n",
    "    \n",
    "    for i, neighbors in enumerate(knn):\n",
    "        for neighbor in neighbors[1:]:  # Skip self-loop\n",
    "            start_edges.append(i)\n",
    "            end_edges.append(neighbor)\n",
    "            start_edges.append(neighbor)  # Add reciprocal edge\n",
    "            end_edges.append(i)\n",
    "\n",
    "    edge_index = torch.tensor([np.array(start_edges), np.array(end_edges)], dtype=torch.long)\n",
    "\n",
    "    return Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "# Example usage:\n",
    "graph = create_mock_stream_graph(mock_streams[0], k=1)\n",
    "print(f\"Graph has {graph.num_nodes} nodes and {graph.num_edges} edges.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd175b84-7993-430e-bb4c-05db11a3b14f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.figure() \n",
    "# # nx.draw(torch_geometric.utils.to_networkx(graph))\n",
    "# nx.draw(torch_geometric.utils.to_networkx(graph), \n",
    "#         cmap='plasma', \n",
    "#         node_color = np.arange(graph.num_nodes),\n",
    "#         with_labels=True,\n",
    "#         font_weight='bold',\n",
    "#         font_color='white',\n",
    "#         node_size=400, linewidths=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f262b40-34d9-481f-b2a7-255f14d5af77",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2 # set number of k-NN neighbors for the edge connectivity\n",
    "train_streams = [create_mock_stream_graph(file, k=k) for file in tqdm(mock_streams[:-20], desc=\"Train set\")]\n",
    "test_streams = [create_mock_stream_graph(file, k=k) for file in tqdm(mock_streams[-20:], desc=\"Test set\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2f58c6-e36e-469c-bac9-ad9fb2b1249b",
   "metadata": {},
   "source": [
    "# Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce307349-e267-4692-b470-7f47e80a68b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Uses GCNConv layers, i.e. graph convolutional layers \n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_channels, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x  # Return raw logits\n",
    "#return F.log_softmax(x, dim=1) # converts into log-probabilities for each class (i.e. not-stream vs. stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da164a79-f2c9-44e8-a95c-43c04a8f3f57",
   "metadata": {},
   "source": [
    "# Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca76e55d-6500-4872-b4f6-9c8f71c92cfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Check for available GPUs\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "### Create a DataLoader\n",
    "train_loader = DataLoader(train_streams, batch_size=32, shuffle=False)\n",
    "\n",
    "### Initialize the model, optimizer, and loss function\n",
    "model = GCN(num_node_features=6, hidden_channels=64, num_classes=2)\n",
    "model = model.to(device) # move onto the GPU\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "liveloss = PlotLosses(figsize=(6, 3)) \n",
    "\n",
    "### Training loop\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    logs = {}\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ### Move the batch onto the GPU\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        loss = criterion(out, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    loss_per_batch = total_loss / len(train_loader)\n",
    "    logs['train_loss'] = loss_per_batch\n",
    "    \n",
    "    liveloss.update(logs)\n",
    "    \n",
    "    liveloss.send()\n",
    "    print(f'Epoch {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c76635-0dc3-4cd2-bc8f-820de1f4be8a",
   "metadata": {},
   "source": [
    "# Evaluate on test streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efab328-b5ad-4e09-9a5b-a955eb11fe94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_plots = True \n",
    "\n",
    "test_loader = DataLoader(test_streams, batch_size=1, shuffle=False) # one stream at a time\n",
    "model.eval()\n",
    "purities = []\n",
    "\n",
    "for batch in test_loader:\n",
    "    ### Move the batch onto the GPU\n",
    "    batch = batch.to(device)\n",
    "    \n",
    "    # print(batch)\n",
    "    df = pd.DataFrame(batch.x.cpu().detach().numpy(), columns=[\"ϕ\", \"λ\", \"μ_ϕcosλ\", \"μ_λ\", 'b-r', 'g'])\n",
    "    df[\"stream\"] = batch.y.cpu().detach().numpy().astype(\"bool\")\n",
    "    out = model(batch.x, batch.edge_index)\n",
    "    df[\"nn_score\"] = out.softmax(dim=1)[:,1].cpu().detach().numpy() # probability that the star is part of a stream\n",
    "\n",
    "    if make_plots: \n",
    "        fig, axs = plt.subplots(dpi=150, ncols=3, figsize=(9,3), tight_layout=True)\n",
    "        axs[0].hist(df.nn_score, bins=25);\n",
    "        axs[0].set_yscale(\"log\")\n",
    "        axs[0].set_title(\"NN Scores\");\n",
    "    \n",
    "        axs[1].scatter(df[\"ϕ\"], df[\"λ\"], c=df.nn_score, cmap=\"inferno\", marker=\".\");\n",
    "        axs[1].set_title(\"All star scores\");\n",
    "        axs[1].set_xlabel(r\"$\\phi$ [$^\\circ$]\")\n",
    "        axs[1].set_ylabel(r\"$\\lambda$ [$^\\circ$]\");\n",
    "\n",
    "    top_stars = df.sort_values('nn_score',ascending=False)[:100]\n",
    "    try:\n",
    "        n_perfect_matches = top_stars.stream.value_counts()[True]\n",
    "        purity = n_perfect_matches/len(top_stars)*100\n",
    "    except:\n",
    "        purity = 0\n",
    "    purities.append(purity)\n",
    "    \n",
    "    if make_plots: \n",
    "        axs[2].scatter(top_stars[top_stars.stream][\"ϕ\"], top_stars[top_stars.stream][\"λ\"], color=\"crimson\", marker=\".\");\n",
    "        axs[2].scatter(top_stars[~top_stars.stream][\"ϕ\"], top_stars[~top_stars.stream][\"λ\"], color=\"lightpink\", marker=\".\");\n",
    "        axs[2].set_xlabel(r\"$\\phi$ [$^\\circ$]\")\n",
    "        axs[2].set_ylabel(r\"$\\lambda$ [$^\\circ$]\");\n",
    "        axs[2].set_title(f\"Top 100 Stars (Purity = {purity:.0f}%)\");\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plt.figure(dpi=200)\n",
    "plt.hist(purities, bins=20);\n",
    "plt.xlabel(\"Purity [%]\");\n",
    "plt.title(\"Test Set Streams\");\n",
    "print(f\"Median purity across all {len(test_loader)} test streams is {np.median(purities):.0f}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cf8ccb-b3d2-4d6e-b7e6-22f29d6c0b25",
   "metadata": {},
   "source": [
    "# Misc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7367c8-c17e-4876-bf82-cd321c61320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### kill off 99.9% of the background stars for now \n",
    "# df_bkg = df[~df.stream].sample(frac=0.001)\n",
    "# df_sig = df[df.stream]\n",
    "# df = pd.concat([df_bkg, df_sig]).sample(frac=1) # shuffle rows\n",
    "\n",
    "### create a rectangular bounding box \n",
    "# def produce_rectangle(df, plot=False):\n",
    "#     stream = df[df.stream]\n",
    "#     delta_ϕ  = (max(stream['ϕ']) - min(stream['ϕ']))/2\n",
    "#     center_ϕ = (max(stream['ϕ']) + min(stream['ϕ']))/2\n",
    "#     delta_λ  = (max(stream['λ']) - min(stream['λ']))/2\n",
    "#     center_λ = (max(stream['λ']) + min(stream['λ']))/2\n",
    "\n",
    "#     box = df[(df[\"ϕ\"] > center_ϕ - delta_ϕ) & (df[\"ϕ\"] < center_ϕ + delta_ϕ)\n",
    "#            & (df[\"λ\"] > center_λ - delta_λ) & (df[\"λ\"] < center_λ + delta_λ)]\n",
    "    \n",
    "#     box.reset_index(inplace=True)\n",
    "    \n",
    "#     if plot:\n",
    "#         plt.scatter(df['ϕ'], df['λ'], color='grey', label='all data')\n",
    "#         plt.scatter(box['ϕ'], box['λ'], color='red', label='bounding box')\n",
    "#         plt.scatter(stream['ϕ'], stream['λ'], color='black', label='stream')\n",
    "#         plt.legend()    \n",
    "        \n",
    "#     return box\n",
    "\n",
    "# box = produce_rectangle(df, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4483ddcd-93d7-417b-8418-efd924da69c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# List to store purity scores per run\n",
    "purity_scores = []\n",
    "all_stream_purities = []\n",
    "\n",
    "# Run 10 separate models\n",
    "for run in range(25):\n",
    "    print(f\"\\nRunning Model {run + 1}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = GCN(num_node_features=6, hidden_channels=64, num_classes=2).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for epoch in range(100):  # Adjust epochs as needed\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch.x, batch.edge_index)\n",
    "            loss = criterion(out, batch.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        #print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "    # Evaluate on the test set focusing on purity\n",
    "    model.eval()\n",
    "    purities = []\n",
    "    stream_purities = []\n",
    "\n",
    "    for idx, batch in enumerate(test_loader):\n",
    "        batch = batch.to(device)\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        preds = out.argmax(dim=1)\n",
    "\n",
    "        # Calculate purity: TP / (TP + FP) for each stream\n",
    "        true_positive = ((preds == 1) & (batch.y == 1)).sum().item()\n",
    "        predicted_positive = (preds == 1).sum().item()\n",
    "        purity = true_positive / predicted_positive if predicted_positive > 0 else 0\n",
    "        purities.append(purity)\n",
    "        stream_purities.append({'Run': run + 1, 'Stream': idx + 1, 'Purity': purity})\n",
    "\n",
    "    avg_purity = np.mean(purities)\n",
    "    purity_scores.append(avg_purity)\n",
    "    all_stream_purities.extend(stream_purities)\n",
    "\n",
    "    print(f\"Model {run + 1} Average Purity: {avg_purity:.4f}\")\n",
    "\n",
    "import os\n",
    "# Calculate and print the average purity over all 10 runs\n",
    "overall_avg_purity = np.mean(purity_scores)\n",
    "print(f\"\\nOverall Average Purity over 10 models: {overall_avg_purity:.4f}\")\n",
    "\n",
    "# Create DataFrame for stream purities\n",
    "stream_purity_df = pd.DataFrame(all_stream_purities)\n",
    "\n",
    "# Define the file path\n",
    "csv_file = 'stream_purities_per_run_25_models.csv'\n",
    "\n",
    "# Check if the file exists before creating it\n",
    "if not os.path.exists(csv_file):\n",
    "    stream_purity_df.to_csv(csv_file, index=False)\n",
    "    print(f\"\\nPer-stream purity data saved to '{csv_file}'\")\n",
    "else:\n",
    "    print(f\"\\nFile '{csv_file}' already exists. No new file was created.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878b1b35-5770-4b46-abb1-a2cbc84944aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (geo)",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
