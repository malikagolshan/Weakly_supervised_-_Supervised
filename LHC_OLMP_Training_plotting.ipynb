{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4274fdd1-aa71-4eb8-9413-63c593213e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 08:51:03.767706: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-25 08:51:03.767742: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-25 08:51:03.768863: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-25 08:51:03.775251: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-25 08:51:05.935543: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 08:51:15.030910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38366 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:03:00.0, compute capability: 8.0\n",
      "2025-04-25 08:51:15.032638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 38366 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:41:00.0, compute capability: 8.0\n",
      "2025-04-25 08:51:15.034408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 38366 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:82:00.0, compute capability: 8.0\n",
      "2025-04-25 08:51:15.037181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 38366 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:c1:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "import tensorflow as tf\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(f\"Number of devices: {strategy.num_replicas_in_sync}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36ede5ae-8d2e-4ed8-b4d2-98766ceff3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1. Plot Overlapping Histograms (Signal vs. Background)\n",
    "###############################################################################\n",
    "def plot_signal_vs_background(df: pd.DataFrame, feature: str = \"mj2\", bins=50):\n",
    "    \"\"\"\n",
    "    Plot overlapping histograms of a given feature for signal vs. background,\n",
    "    normalized to unit area.\n",
    "    \"\"\"\n",
    "    df_signal = df[df[\"label\"] == 1]\n",
    "    df_background = df[df[\"label\"] == 0]\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(df_signal[feature], bins=bins, density=True,\n",
    "             histtype='step', label='signal')\n",
    "    plt.hist(df_background[feature], bins=bins, density=True,\n",
    "             histtype='step', label='background')\n",
    "\n",
    "    plt.xlabel(f\"{feature} [TeV]\")\n",
    "    plt.ylabel(\"Fraction of Events / bin\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a597578-bd58-4366-8bd3-b6916965d910",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 2. Load from HDF5: ensures m, mx, my, tau12, tau23, label exist\n",
    "###############################################################################\n",
    "def load_m_mx_my_tau12_tau23(\n",
    "    file_path: str,\n",
    "    key: str = \"/df\",\n",
    "    scale_to_tev: bool = False,\n",
    "    assign_label: int = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads HDF5 (file_path) under group 'key' into a DataFrame.\n",
    "    - If 'assign_label' is an integer, sets 'label' to that.\n",
    "    - If 'scale_to_tev' is True, multiply masses by 0.001.\n",
    "    - Ensures columns: mj1, mj2, tau12j1, tau23j1, tau12j2, tau23j2, mx, my, label.\n",
    "    \"\"\"\n",
    "    df = pd.read_hdf(file_path, key=key)\n",
    "\n",
    "    # Assign label \n",
    "    if assign_label is not None:\n",
    "        df[\"label\"] = assign_label\n",
    "\n",
    "    # scale from GeV to TeV\n",
    "    scale = 0.001 if scale_to_tev else 1.0\n",
    "\n",
    "    for j in [\"j1\", \"j2\"]:\n",
    "        mcol = f\"m{j}\"\n",
    "        if mcol not in df.columns:\n",
    "            df[mcol] = 0.0\n",
    "        df[mcol] *= scale\n",
    "\n",
    "        for base in [\"tau1\", \"tau2\", \"tau3\"]:\n",
    "            if f\"{base}{j}\" not in df.columns:\n",
    "                df[f\"{base}{j}\"] = 0.0\n",
    "\n",
    "        tau1 = df[f\"tau1{j}\"]\n",
    "        tau2 = df[f\"tau2{j}\"]\n",
    "        tau3 = df[f\"tau3{j}\"]\n",
    "        mask_12 = (tau1 > 0) & (tau2 > 0)\n",
    "        mask_23 = (tau2 > 0) & (tau3 > 0)\n",
    "        df[f\"tau12{j}\"] = np.where(mask_12, tau2 / tau1, 0.0)\n",
    "        df[f\"tau23{j}\"] = np.where(mask_23, tau3 / tau2, 0.0)\n",
    "\n",
    "    if \"mx\" not in df.columns:\n",
    "        df[\"mx\"] = 0.0\n",
    "    if \"my\" not in df.columns:\n",
    "        df[\"my\"] = 0.0\n",
    "\n",
    "    needed_cols = [\n",
    "        \"mj1\", \"tau12j1\", \"tau23j1\",\n",
    "        \"mj2\", \"tau12j2\", \"tau23j2\",\n",
    "        \"mx\", \"my\"\n",
    "    ]\n",
    "    if \"label\" in df.columns:\n",
    "        needed_cols.append(\"label\")\n",
    "\n",
    "    \n",
    "    for c in needed_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = 0.0\n",
    "\n",
    "    return df[needed_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db0b9397-ba8a-4224-84aa-7bc0e2c2d494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from aliad.interface.tensorflow.losses import ScaledBinaryCrossentropy  \n",
    "import numpy as np\n",
    "\n",
    "def base_highlevel_model(name=\"BaseModel\", input_name=\"input_features\", input_shape=(2, 3), use_scaled_bce=False):\n",
    "    inputs = Input(shape=input_shape, name=input_name)\n",
    "    x = Flatten()(inputs)\n",
    "\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs, outputs, name=name)\n",
    "\n",
    "    if use_scaled_bce:\n",
    "        loss_fn = ScaledBinaryCrossentropy(offset=-np.log(2), scale=1000)\n",
    "    else:\n",
    "        loss_fn = \"binary_crossentropy\"\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-3), loss=loss_fn, metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def simple_supervised_model():\n",
    "    return base_highlevel_model(name=\"SimpleSupervisedModel\", input_name=\"input_features\", use_scaled_bce=False)\n",
    "\n",
    "def ideal_weakly_supervised_model():\n",
    "    return base_highlevel_model(name=\"IdealWeaklySupervisedModel\", input_name=\"ideal_weakly_input\", use_scaled_bce=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02725479-a9c0-4556-a864-81f72f4af78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 4. Exclude Specific Mass Points\n",
    "###############################################################################\n",
    "def exclude_mass_points(df: pd.DataFrame, mass_pairs_to_exclude):\n",
    "    \"\"\"\n",
    "    Splits 'df' into included vs. excluded data based on (mx, my).\n",
    "    \"\"\"\n",
    "    mass_set = set(mass_pairs_to_exclude)\n",
    "    df[\"excluded\"] = df.apply(lambda r: (r[\"mx\"], r[\"my\"]) in mass_set, axis=1)\n",
    "    df_excluded = df[df[\"excluded\"]].copy()\n",
    "    df_included = df[~df[\"excluded\"]].copy()\n",
    "\n",
    "    df.drop(columns=[\"excluded\"], inplace=True)\n",
    "    df_included.drop(columns=[\"excluded\"], inplace=True)\n",
    "    df_excluded.drop(columns=[\"excluded\"], inplace=True)\n",
    "    return df_included, df_excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "100e8a66-6c0c-4036-a4a4-d8ce8cddc2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def plot_mj2_histograms(df_signal, df_background):\n",
    "    mj2_signal = df_signal[\"mj2\"].values\n",
    "    mj2_background = df_background[\"mj2\"].values\n",
    "    bins = np.linspace(0, 1, 1000)\n",
    "    plt.hist(mj2_signal, bins=bins, density=True, histtype='step', color='black', label='signal', linewidth=1.5)\n",
    "    plt.hist(mj2_background, bins=bins, density=True, histtype='stepfilled', color='crimson', alpha=0.5, label='background')\n",
    "    plt.xlabel(r'$m_{j2}$ [TeV]', fontsize=14)\n",
    "    plt.ylabel(f'Fraction of Events / {np.round(np.diff(bins)[0], 5)}', fontsize=14)\n",
    "    plt.legend(frameon=False, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "def compute_sic(y_true, y_score):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        sic = tpr / np.sqrt(fpr)\n",
    "        sic[np.isnan(sic)] = 0.0\n",
    "        sic[np.isinf(sic)] = 0.0\n",
    "    return thresholds, sic\n",
    "\n",
    "def plot_sic_curves_grouped(models_and_preds, title_prefix):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for label, y_true, y_score in models_and_preds:\n",
    "        thresholds, sic = compute_sic(y_true, y_score)\n",
    "        plt.plot(thresholds, sic, label=label)\n",
    "        max_idx = np.argmax(sic)\n",
    "        plt.scatter(thresholds[max_idx], sic[max_idx], marker=\"x\", color=\"black\")\n",
    "        plt.text(thresholds[max_idx], sic[max_idx], f\"{sic[max_idx]:.2f}\", fontsize=8)\n",
    "    plt.title(f\"SIC Curve — {title_prefix}\")\n",
    "    plt.xlabel(\"NN Score Threshold\")\n",
    "    plt.ylabel(\"SIC (TPR / √FPR)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def train_and_predict(model_fn, model_key, df_signal, df_background, df_test_true, epochs=10, is_weakly=False, signal_fraction=0.001):\n",
    "    #model = model_fn()\n",
    "    with strategy.scope():\n",
    "        model = model_fn()\n",
    "        model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    outdir = \"./my_trained_models\"\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    best_path = os.path.join(outdir, f\"best_{model_key}.keras\")\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1)\n",
    "    checkpoint = ModelCheckpoint(filepath=best_path, monitor=\"val_loss\", save_best_only=True, verbose=1)\n",
    "\n",
    "    df_true_test = df_test_true.sample(frac=1.0, random_state=42)\n",
    "    X_test_true = df_true_test[[\"mj1\", \"tau12j1\", \"tau23j1\", \"mj2\", \"tau12j2\", \"tau23j2\"]].values.reshape(-1, 2, 3)\n",
    "    y_test_true = df_true_test[\"label\"].values.astype(np.float32)\n",
    "\n",
    "    if is_weakly:\n",
    "        df_bg = df_background.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "        half = len(df_bg) // 2\n",
    "        R = df_bg.iloc[:half].copy()\n",
    "        D_bg = df_bg.iloc[half:].copy()\n",
    "        n_signal = int(len(D_bg) * signal_fraction)\n",
    "        D_sig = df_signal.sample(n=n_signal, random_state=42)\n",
    "        D = pd.concat([D_bg, D_sig], ignore_index=True)\n",
    "        R[\"label\"] = 0\n",
    "        D[\"label\"] = 1\n",
    "        df_weak = pd.concat([R, D], ignore_index=True).sample(frac=1.0, random_state=42)\n",
    "        X_weak = df_weak[[\"mj1\", \"tau12j1\", \"tau23j1\", \"mj2\", \"tau12j2\", \"tau23j2\"]].values.reshape(-1, 2, 3)\n",
    "        y_weak = df_weak[\"label\"].values.astype(np.float32)\n",
    "\n",
    "        model.fit(X_weak, y_weak,\n",
    "                  validation_split=0.20,\n",
    "                  epochs=epochs, batch_size=64,\n",
    "                  callbacks=[early_stopping, checkpoint], verbose=1)\n",
    "\n",
    "    else:\n",
    "        df = pd.concat([df_signal, df_background], ignore_index=True)\n",
    "        X = df[[\"mj1\", \"tau12j1\", \"tau23j1\", \"mj2\", \"tau12j2\", \"tau23j2\"]].values.reshape(-1, 2, 3)\n",
    "        y = df[\"label\"].values.astype(np.float32)\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, stratify=y, random_state=42)\n",
    "\n",
    "        model.fit(X_train, y_train,\n",
    "                  validation_data=(X_val, y_val),\n",
    "                  epochs=epochs, batch_size=64,\n",
    "                  callbacks=[early_stopping, checkpoint], verbose=1)\n",
    "\n",
    "    y_pred = model.predict(X_test_true).flatten()\n",
    "    return y_test_true, y_pred\n",
    "\n",
    "def main():\n",
    "    qcd_file = \"/global/cfs/projectdirs/m3246/alkaid/paws/datasets/original/events_anomalydetection_v2.features.h5\"\n",
    "    extra_qcd_file = \"/global/cfs/projectdirs/m3246/alkaid/paws/datasets/original/events_anomalydetection_qcd_extra_inneronly_features.h5\"\n",
    "    w_qq_file = \"/global/cfs/projectdirs/m3246/alkaid/paws/datasets/original/events_anomalydetection_Z_XY_qq_parametric.h5\"\n",
    "\n",
    "    df_qcd = load_m_mx_my_tau12_tau23(qcd_file, key=\"/df\", scale_to_tev=True)\n",
    "    df_extra = load_m_mx_my_tau12_tau23(extra_qcd_file, key=\"/df\", scale_to_tev=True)\n",
    "    df_signal = load_m_mx_my_tau12_tau23(w_qq_file, key=\"/output\", scale_to_tev=True)\n",
    "\n",
    "    df_qcd[\"label\"] = 0\n",
    "    df_extra[\"label\"] = 0\n",
    "    df_signal[\"label\"] = 1\n",
    "\n",
    "    df_background = pd.concat([df_qcd, df_extra], ignore_index=True)\n",
    "\n",
    "    signal_300 = df_signal.query(\"mx == 300 and my == 300\")\n",
    "    signal_100_500 = df_signal.query(\"mx == 100 and my == 500\")\n",
    "    signal_500_100 = df_signal.query(\"mx == 500 and my == 100\")\n",
    "\n",
    "    df_signal_general = df_signal.query(\"not ((mx == 300 and my == 300) or \\\n",
    "                                              (mx == 100 and my == 500) or \\\n",
    "                                              (mx == 500 and my == 100))\")\n",
    "\n",
    "    background_test = pd.concat([\n",
    "        df_qcd.sample(n=2500, random_state=1),\n",
    "        df_extra.sample(n=2500, random_state=2)\n",
    "    ])\n",
    "    testset_300 = pd.concat([background_test, signal_300.sample(n=2500, random_state=3)])\n",
    "    testset_100_500 = pd.concat([background_test, signal_100_500.sample(n=2500, random_state=4)])\n",
    "\n",
    "    model_defs = [\n",
    "        (\"Simple Supervised (All)\", simple_supervised_model, \"Simple_All\", df_signal_general, False),\n",
    "        (\"Simple Supervised (300_300)\", simple_supervised_model, \"Simple_300_300\", signal_300, False),\n",
    "        (\"Simple Supervised (100_500)\", simple_supervised_model, \"Simple_100_500\", signal_100_500, False),\n",
    "    ]\n",
    "\n",
    "    weakly_defs = [\n",
    "        (\"Ideal Weakly (300_300)\", ideal_weakly_supervised_model, \"Weakly_300_300\", signal_300, True),\n",
    "        (\"Ideal Weakly (100_500)\", ideal_weakly_supervised_model, \"Weakly_100_500\", signal_100_500, True),\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    signal_fracs = [0.001, 0.01, 0.005]\n",
    "\n",
    "    for label, model_fn, key, signal_df, is_weak in model_defs:\n",
    "        # Train once\n",
    "        _ = train_and_predict(model_fn, key, signal_df, df_background, testset_300, is_weakly=is_weak)\n",
    "\n",
    "        # Load trained weights\n",
    "        model = model_fn()\n",
    "        model.load_weights(f\"./my_trained_models/best_{key}.keras\")\n",
    "\n",
    "        # Predict on testset_300\n",
    "        X_300 = testset_300[[\"mj1\", \"tau12j1\", \"tau23j1\", \"mj2\", \"tau12j2\", \"tau23j2\"]].values.reshape(-1, 2, 3)\n",
    "        y_300 = testset_300[\"label\"].values.astype(np.float32)\n",
    "        y_pred_300 = model.predict(X_300).flatten()\n",
    "        results.append((f\"{label} on 300_300\", y_300, y_pred_300))\n",
    "\n",
    "        # Predict on testset_100_500\n",
    "        X_100_500 = testset_100_500[[\"mj1\", \"tau12j1\", \"tau23j1\", \"mj2\", \"tau12j2\", \"tau23j2\"]].values.reshape(-1, 2, 3)\n",
    "        y_100_500 = testset_100_500[\"label\"].values.astype(np.float32)\n",
    "        y_pred_100_500 = model.predict(X_100_500).flatten()\n",
    "        results.append((f\"{label} on 100_500\", y_100_500, y_pred_100_500))\n",
    "\n",
    "    for signal_frac in signal_fracs:\n",
    "        for label, model_fn, key, signal_df, is_weak in weakly_defs:\n",
    "            key_frac = f\"{key}_frac{signal_frac:.3f}\".replace(\"0.\", \"\")\n",
    "            label_frac = f\"{label} μ={signal_frac:.3f}\"\n",
    "\n",
    "            y_true_300, y_pred_300 = train_and_predict(model_fn, key_frac, signal_df, df_background, testset_300, is_weakly=is_weak, signal_fraction=signal_frac)\n",
    "            results.append((f\"{label_frac} on 300_300\", y_true_300, y_pred_300))\n",
    "\n",
    "            y_true_100_500, y_pred_100_500 = train_and_predict(model_fn, key_frac, signal_df, df_background, testset_100_500, is_weakly=is_weak, signal_fraction=signal_frac)\n",
    "            results.append((f\"{label_frac} on 100_500\", y_true_100_500, y_pred_100_500))\n",
    "\n",
    "    plot_sic_curves_grouped(\n",
    "    [r for r in results if \"on 300_300\" in r[0] and not (\"Ideal Weakly (100_500)\" in r[0])],\n",
    "    title_prefix=\"Mass Point (300, 300)\")\n",
    "\n",
    "    plot_sic_curves_grouped(\n",
    "    [r for r in results if \"on 100_500\" in r[0] and not (\"Ideal Weakly (300_300)\" in r[0])],\n",
    "    title_prefix=\"Mass Point (100, 500)\")\n",
    "\n",
    "\n",
    "    # === Start of Figure 2-style panel ===\n",
    "    injection_levels = np.arange(0.001, 0.011, 0.001)\n",
    "    mass_points = [(100, 500), (300, 300)]\n",
    "    results_dict = {}\n",
    "\n",
    "    for mx, my in mass_points:\n",
    "        if (mx, my) == (100, 500):\n",
    "            signal_df = signal_100_500\n",
    "            test_df = testset_100_500\n",
    "        elif (mx, my) == (300, 300):\n",
    "            signal_df = signal_300\n",
    "            test_df = testset_300\n",
    "\n",
    "        results_dict[(mx, my)] = {\"mu\": [], \"sic_median\": [], \"sic_std\": [], \"param_median\": [], \"param_std\": []}\n",
    "\n",
    "        for mu in injection_levels:\n",
    "            sic_vals = []\n",
    "            param_vals = []\n",
    "\n",
    "            for seed in range(5):  # 5 retrainings\n",
    "                np.random.seed(seed)\n",
    "                tf.random.set_seed(seed)\n",
    "\n",
    "                model = ideal_weakly_supervised_model()\n",
    "                y_true, y_pred = train_and_predict(\n",
    "                    model_fn=ideal_weakly_supervised_model,\n",
    "                    model_key=f\"mx{mx}_my{my}_mu{mu:.3f}_seed{seed}\",\n",
    "                    df_signal=signal_df,\n",
    "                    df_background=df_background,\n",
    "                    df_test_true=test_df,\n",
    "                    is_weakly=True,\n",
    "                    signal_fraction=mu\n",
    "                )\n",
    "\n",
    "                # Compute SIC at fixed εB = 0.001\n",
    "                fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "                try:\n",
    "                    idx = np.where(fpr >= 0.001)[0][0]\n",
    "                    sic = tpr[idx] / np.sqrt(fpr[idx])\n",
    "                except IndexError:\n",
    "                    sic = 0\n",
    "                sic_vals.append(sic)\n",
    "\n",
    "                # Summary stat — here: mean of NN output\n",
    "                param_vals.append(np.mean(y_pred))\n",
    "\n",
    "            results_dict[(mx, my)][\"mu\"].append(mu)\n",
    "            results_dict[(mx, my)][\"sic_median\"].append(np.median(sic_vals))\n",
    "            results_dict[(mx, my)][\"sic_std\"].append(np.std(sic_vals))\n",
    "            results_dict[(mx, my)][\"param_median\"].append(np.median(param_vals))\n",
    "            results_dict[(mx, my)][\"param_std\"].append(np.std(param_vals))\n",
    "\n",
    "    # === Plot Figure 2-style panel ===\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 8), sharex=True)\n",
    "\n",
    "    for i, (mx, my) in enumerate(mass_points):\n",
    "        data = results_dict[(mx, my)]\n",
    "        mu = np.array(data[\"mu\"])\n",
    "\n",
    "        # Top: SIC\n",
    "        ax_top = axs[0, i]\n",
    "        ax_top.plot(mu, data[\"sic_median\"], label='Ideal Weak', color='blue')\n",
    "        ax_top.fill_between(mu,\n",
    "                            np.array(data[\"sic_median\"]) - np.array(data[\"sic_std\"]),\n",
    "                            np.array(data[\"sic_median\"]) + np.array(data[\"sic_std\"]),\n",
    "                            alpha=0.3, color='blue')\n",
    "        ax_top.set_title(f\"(mX, mY) = ({mx}, {my})\")\n",
    "        ax_top.set_ylabel(\"SIC at εB = 0.1%\")\n",
    "        ax_top.grid(True)\n",
    "\n",
    "        # Bottom: Predicted param (NN output in this case)\n",
    "        ax_bot = axs[1, i]\n",
    "        ax_bot.plot(mu, data[\"param_median\"], label='Predicted Value', color='green')\n",
    "        ax_bot.fill_between(mu,\n",
    "                            np.array(data[\"param_median\"]) - np.array(data[\"param_std\"]),\n",
    "                            np.array(data[\"param_median\"]) + np.array(data[\"param_std\"]),\n",
    "                            alpha=0.3, color='green')\n",
    "        ax_bot.set_xlabel(\"Signal Injection Fraction μ\")\n",
    "        ax_bot.set_ylabel(\"NN Output (avg)\")\n",
    "        ax_bot.axhline(1.0, linestyle=\"--\", color=\"gray\", label=\"True Value\")\n",
    "        ax_bot.grid(True)\n",
    "\n",
    "    axs[0, 0].legend()\n",
    "    axs[1, 0].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def main_figure3_style_side_by_side():\n",
    "    df_qcd = load_m_mx_my_tau12_tau23(\n",
    "        \"/global/cfs/projectdirs/m3246/alkaid/paws/datasets/original/events_anomalydetection_v2.features.h5\",\n",
    "        key=\"/df\", scale_to_tev=True\n",
    "    )\n",
    "    df_extra = load_m_mx_my_tau12_tau23(\n",
    "        \"/global/cfs/projectdirs/m3246/alkaid/paws/datasets/original/events_anomalydetection_qcd_extra_inneronly_features.h5\",\n",
    "        key=\"/df\", scale_to_tev=True\n",
    "    )\n",
    "    df_signal = load_m_mx_my_tau12_tau23(\n",
    "        \"/global/cfs/projectdirs/m3246/alkaid/paws/datasets/original/events_anomalydetection_Z_XY_qq_parametric.h5\",\n",
    "        key=\"/output\", scale_to_tev=True\n",
    "    )\n",
    "\n",
    "    df_qcd[\"label\"] = 0\n",
    "    df_extra[\"label\"] = 0\n",
    "    df_signal[\"label\"] = 1\n",
    "    df_background = pd.concat([df_qcd, df_extra], ignore_index=True)\n",
    "\n",
    "    background_test = pd.concat([\n",
    "        df_qcd.sample(n=2500, random_state=1),\n",
    "        df_extra.sample(n=2500, random_state=2)\n",
    "    ])\n",
    "    signal_300 = df_signal.query(\"mx == 300 and my == 300\")\n",
    "    signal_100_500 = df_signal.query(\"mx == 100 and my == 500\")\n",
    "\n",
    "    testsets = {\n",
    "        (300, 300): pd.concat([background_test, signal_300.sample(n=2500, random_state=3)]),\n",
    "        (100, 500): pd.concat([background_test, signal_100_500.sample(n=2500, random_state=4)])\n",
    "    }\n",
    "\n",
    "    signal_sets = {\n",
    "        (300, 300): signal_300,\n",
    "        (100, 500): signal_100_500\n",
    "    }\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n",
    "\n",
    "    for i, (mx, my) in enumerate([(300, 300), (100, 500)]):\n",
    "        ax = axs[i]\n",
    "        testset = testsets[(mx, my)]\n",
    "        sig_df = signal_sets[(mx, my)]\n",
    "        X_test = testset[[\"mj1\", \"tau12j1\", \"tau23j1\", \"mj2\", \"tau12j2\", \"tau23j2\"]].values.reshape(-1, 2, 3)\n",
    "        y_true = testset[\"label\"].values.astype(np.float32)\n",
    "\n",
    "        # === Train simple supervised model ===\n",
    "        df = pd.concat([sig_df, df_background], ignore_index=True)\n",
    "        X = df[[\"mj1\", \"tau12j1\", \"tau23j1\", \"mj2\", \"tau12j2\", \"tau23j2\"]].values.reshape(-1, 2, 3)\n",
    "        y = df[\"label\"].values.astype(np.float32)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "        model_sup = simple_supervised_model()\n",
    "        model_sup.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=512, verbose=0)\n",
    "        y_pred_sup = model_sup.predict(X_test).flatten()\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_pred_sup)\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            sic = tpr / np.sqrt(fpr)\n",
    "            sic[np.isnan(sic)] = 0.0\n",
    "            sic[np.isinf(sic)] = 0.0\n",
    "        ax.plot(tpr, sic, label=\"Simple Supervised\")\n",
    "\n",
    "        # === Train ideal weakly supervised model ===\n",
    "        df_bg = df_background.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "        half = len(df_bg) // 2\n",
    "        R = df_bg.iloc[:half].copy()\n",
    "        D_bg = df_bg.iloc[half:].copy()\n",
    "        n_signal = int(len(D_bg) * 0.001)\n",
    "        D_sig = sig_df.sample(n=n_signal, random_state=42)\n",
    "        D = pd.concat([D_bg, D_sig], ignore_index=True)\n",
    "        R[\"label\"] = 0\n",
    "        D[\"label\"] = 1\n",
    "        df_weak = pd.concat([R, D], ignore_index=True).sample(frac=1.0, random_state=42)\n",
    "        X_weak = df_weak[[\"mj1\", \"tau12j1\", \"tau23j1\", \"mj2\", \"tau12j2\", \"tau23j2\"]].values.reshape(-1, 2, 3)\n",
    "        y_weak = df_weak[\"label\"].values.astype(np.float32)\n",
    "\n",
    "        model_weak = ideal_weakly_supervised_model()\n",
    "        model_weak.fit(X_weak, y_weak, validation_split=0.2, epochs=10, batch_size=512, verbose=0)\n",
    "        y_pred_weak = model_weak.predict(X_test).flatten()\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_pred_weak)\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            sic = tpr / np.sqrt(fpr)\n",
    "            sic[np.isnan(sic)] = 0.0\n",
    "            sic[np.isinf(sic)] = 0.0\n",
    "        ax.plot(tpr, sic, label=\"Ideal Weakly Supervised\", linestyle=\"--\")\n",
    "\n",
    "        ax.set_title(f\"(mX, mY) = ({mx}, {my})\", fontsize=13)\n",
    "        ax.set_xlabel(\"Signal Efficiency\", fontsize=12)\n",
    "        if i == 0:\n",
    "            ax.set_ylabel(\"Significance Improvement\", fontsize=12)\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c912110d-ae2d-4fd9-a511-1a41ec61c799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "INFO:tensorflow:Collective all_reduce tensors: 8 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Collective all_reduce tensors: 8 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 08:51:35.019643: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f43544365a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-04-25 08:51:35.019664: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2025-04-25 08:51:35.019676: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2025-04-25 08:51:35.019681: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2025-04-25 08:51:35.019694: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (3): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2025-04-25 08:51:35.029368: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-04-25 08:51:35.082774: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8903\n",
      "2025-04-25 08:51:35.084451: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8903\n",
      "2025-04-25 08:51:35.086789: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8903\n",
      "2025-04-25 08:51:35.090240: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8903\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745596295.178112 1252623 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1864/170510 [..............................] - ETA: 11:52 - loss: 0.2398 - accuracy: 0.9104    "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    main_figure3_style_side_by_side()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ade0eb9-9342-4545-a5fc-69cd62ad3dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-2.15.0",
   "language": "python",
   "name": "tensorflow-2.15.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
